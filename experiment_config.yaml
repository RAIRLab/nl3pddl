# This is the configuration file for the experiment

# Misc Settings ===============================================================

# Used for cost estimation of experiments.
price:
  o4-mini:
    input: 4.0e-6
    output: 1.6e-5
  deepseek-chat:
    input: 2.8e-7
    output: 4.2e-7
  deepseek-reasoner:
    input: 2.8e-7
    output: 4.2e-7
    
# Average token usage and call counts for cost estimation
avg-tokens:
  input: 459      # average input tokens per model call (estimated from blocks)
  output: 190     # average output tokens per model call (estimated from blocks)

avg-calls-per-experiment: 10  # average number of model calls per experiment

# Problem Generation Parameters ================================================

# Number of problems to generate that will be used during the feedback loop
feedback-problems: 5

# Number of problems that will be generated for use during the final evaluation
evaluation-problems: 10

# Maximum number of plans to generate per feedback and evaluation problem:
plans-per-problem: 2

# General Experiment Config Parameters =========================================

# This flag is used to test the experiment setup without actually running
# any experiments. When set to true, the experiment will skip running and only
# the parallelism will be tested.
skip-experiment: false 

# Evaluate the generated domains against the evaluation problems every single step, and log 
# the results to the csv file.
always-evaluate: true

# The level to log at:
# See https://docs.python.org/3/library/logging.html#logging-levels
# 10 for DEBUG, which contains our lowest level of logging
# 50 for CRITICAL, which contains no messages
# log-level: 30
log-level: 10

# The maximum number of threads to use for parallel processing.
# If set to 0 or less, it will default to the number of available CPU cores.
threads: -1
#threads: 50

# Maximum number of reprompts to allow for each action generation step.
action-threshold: 5

# Maximum number of reprompts to refine the domain.
hde-threshold: 10

# How many times the entire parameter grid is repeated with randomized seeds 
# for LLM generation. Used for statistical significance.
trials: 5

# Timeout for KStar in seconds (Used during landmark evaluation)
kstar-timeout: 5

# When using KStar for landmark evaluation, up to how many plans should be generated?
kstar-n-plans: 100

# Grid Parameters ==============================================================
# The following parameters define a parameter grid to evaluate over.
# Every possible combination of these parameters will be evaluated over

domains:
  - bookseller
  - checkers-jumping
  - flow
  - hiking
  - pacman-63 
  - keygrid 
  - light-bubble
  # - pacman-72 # Still very broken do not use
  # - sudoku # Generator generates unparsable problems.
  # - sliding-puzzle # Generator does not generate problems.


  
# What models are we evaluating?
# This can be any valid provider and model pair, assuming the API key is set in
# the ENV file and passed in.
models:
  # openai:
    # - o4-mini
    #    - gpt-4o-mini
    # - gpt-5-nano
  deepseek:
    - deepseek-chat
  #  - deepseek-reasoner
  #huggingface:
  #       - "Qwen/Qwen2.5-7B-Instruct"
  #- NousResearch/Hermes-4-405B

# Description classes to evaluate over, should correspond to the classes
# present in the dataset/*/nl.json files.
description-classes:
    - first
    - detailed-first

# list of feedback pipelines to evaluate over.
# - [] The empty pipeline signifies no feedback, just generate the domain once.
# - "landmark generates landmark feedback messages"
# - "validate generates validation feedback messages"
# - "random-single" will randomly select a single message from the set of 
#    proceeding feedback messages
feedback-pipelines:
    - []
    - ["landmark"]
    - ["validate"]
    - ["landmark", "validate"]
    - ["landmark", "random-single"]
    - ["validate", "random-single"]
    - ["landmark", "validate", "random-single"]

# Specifies the search heuristic to use when generating plans.
# Should be a string with valid python expression that evaluates to a number
# involving G distance from the start node and H (heuristic value).
# Only used if feedback-style is set to 'heuristic-search'.
search-heuristic:
#    - "H"
    - "G + H"

# Should the model be presented with a description of predicates from the
# problem set when initially generating the domain?
give-pred-description: [true]
