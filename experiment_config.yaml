# This is the configuration file for the experiment
# Domains are strictly determined by what is present in the data/domains folder!

# Problem Generation Parameters ================================================

# Number of problems to generate that will be used during the feedback loop
feedback-problems: 5

# Number of problems that will be generated for use during the final evaluation
evaluation-problems: 10

# Maximum number of plans to generate per feedback and evaluation problem:
plans-per-problem: 2

# General Experiment Config Parameters =========================================

# The level to log at:
# See https://docs.python.org/3/library/logging.html#logging-levels
# 10 for DEBUG, which contains our lowest level of logging
# 50 for CRITICAL, which contains no messages
# log-level: 30
log-level: 10

# The maximum number of threads to use for parallel processing.
# If set to 0 or less, it will default to the number of available CPU cores.
#threads: 1
threads: 50

# Maximum number of reprompts to allow for each action generation step.
action-threshold: 5

# Maximum number of reprompts to refine the domain.
hde-threshold: 10

# How many times the entire parameter grid is repeated with randomized seeds 
# for LLM generation. Used for statistical significance.
trials: 1

# Timeout for KStar in seconds (Used during landmark evaluation)
kstar-timeout: 5

# When using KStar for landmark evaluation, up to how many plans should be generated?
kstar-n-plans: 100

# Grid Parameters ==============================================================
# The following parameters define a parameter grid to evaluate over.
# Every possible combination of these parameters will be evaluated over
# every single domain in the dataset/domains folder.

# What models are we evaluating?
# This can be any valid provider and model pair, assuming the API key is set in
# the ENV file and passed in.
models:
    openai:
    #    - o4-mini
    #    - gpt-4o-mini
        - gpt-5-nano    
    # deepseek:
    #     - deepseek-chat

# Description classes to evaluate over, should correspond to the classes
# present in the dataset/*/nl.json files.
description-classes:
#    - first
    - detailed-first

# list of feedback pipelines to evaluate. Pipeline stage names correspond to 
# langgraph node names in the experiment.py file.
# TODO: currently the order of the stages is hardcoded, changing the order here
# will not change the order of the stages in the pipeline.
feedback-pipelines:
    #- ["landmark"]
    #- ["validate"]
    - ["landmark", "validate"]


# Should the model be presented with a description of predicates from the
# problem set when initially generating the domain?
give-pred-description: [true]


