# This is the configuration file for the experiment
# Domains are strictly determined by what is present in the data/domains folder!

# Problem Generation Parameters ================================================

# Number of problems to generate that will be used during the feedback loop
feedback-problems: 5

# Number of problems that will be generated for use during the final evaluation
evaluation-problems: 10

# Maximum number of plans to generate per feedback and evaluation problem:
plans-per-problem: 2

# General Experiment Config Parameters =========================================

# The level to log at:
# See https://docs.python.org/3/library/logging.html#logging-levels
# 10 for DEBUG, which contains our lowest level of logging
# 50 for CRITICAL, which contains no messages
# log-level: 30
log-level: 10

# The maximum number of threads to use for parallel processing.
# If set to 0 or less, it will default to the number of available CPU cores.
threads: -1
#threads: 50

# Maximum number of reprompts to allow for each action generation step.
action-threshold: 5

# Maximum number of reprompts to refine the domain.
hde-threshold: 5

# How many times the entire parameter grid is repeated with randomized seeds 
# for LLM generation. Used for statistical significance.
trials: 1

# Timeout for KStar in seconds (Used during landmark evaluation)
kstar-timeout: 5

# When using KStar for landmark evaluation, up to how many plans should be generated?
kstar-n-plans: 100

# Grid Parameters ==============================================================
# The following parameters define a parameter grid to evaluate over.
# Every possible combination of these parameters will be evaluated over
# every single domain in the dataset/domains folder.

# What models are we evaluating?
# This can be any valid provider and model pair, assuming the API key is set in
# the ENV file and passed in.
models:
  openai:
    #- o4-mini
    #    - gpt-4o-mini
          - gpt-5-nano
  #deepseek:
  #        - deepseek-chat
  #huggingface:
  #       - "Qwen/Qwen2.5-7B-Instruct"
    #- NousResearch/Hermes-4-405B

#Supported model providers are: together, azure_openai, groq, ibm, azure_ai, deepseek, bedrock, google_anthropic_vertex, google_genai, o
#â”‚llama, cohere, xai, huggingface, perplexity, anthropic, openai, fireworks, mistralai, google_vertexai, bedrock_converse

# Description classes to evaluate over, should correspond to the classes
# present in the dataset/*/nl.json files.
description-classes:
    - first
#    - detailed-first

# list of feedback pipelines to evaluate. Pipeline stage names correspond to 
# Pipelines effect the creation of the domain after initial generation.
feedback-pipelines:
    - ["landmark"]
#    - ["validate"]
#    - ["landmark", "validate"]

# Specifies the search heuristic to use when generating plans.
# Should be a string with valid python expression that evaluates to a number
# involving G distance from the start node and H (heuristic value).
search-heuristic:
    #- "H"
    - "G + H"
    #- "0.1 * G + H"

# Should the model be presented with a description of predicates from the
# problem set when initially generating the domain?
give-pred-description: [true]


