# This is the configuration file for the experiment
# Domains are strictly determined by what is present in the data/domains folder!

# Problem Generation Parameters ================================================

# Number of problems to generate that will be used during the feedback loop
feedback-problems: 5

# Number of problems that will be generated for use during the final evaluation
evaluation-problems: 10

# Maximum number of plans to generate per feedback and evaluation problem:
plans-per-problem: 2

# General Experiment Config Parameters =========================================

# This flag is used to test the experiment setup without actually running
# any experiments. When set to true, the experiment will skip running and only
# the parallelism will be tested.
skip-experiment: false 

# The level to log at:
# See https://docs.python.org/3/library/logging.html#logging-levels
# 10 for DEBUG, which contains our lowest level of logging
# 50 for CRITICAL, which contains no messages
# log-level: 30
log-level: 10

# The maximum number of threads to use for parallel processing.
# If set to 0 or less, it will default to the number of available CPU cores.
threads: -1
#threads: 50

# Maximum number of reprompts to allow for each action generation step.
action-threshold: 5

# Maximum number of reprompts to refine the domain.
hde-threshold: 10

# How many times the entire parameter grid is repeated with randomized seeds 
# for LLM generation. Used for statistical significance.
trials: 5

# Timeout for KStar in seconds (Used during landmark evaluation)
kstar-timeout: 5

# When using KStar for landmark evaluation, up to how many plans should be generated?
kstar-n-plans: 100

# Grid Parameters ==============================================================
# The following parameters define a parameter grid to evaluate over.
# Every possible combination of these parameters will be evaluated over

domains:
  - bookseller
  - checkers-jumping
  - flow
  - hiking
  - pacman-63
  # - sudoku
  # - bloxorz
  # - pacman-72
  # - keygrid
  
# What models are we evaluating?
# This can be any valid provider and model pair, assuming the API key is set in
# the ENV file and passed in.
models:
  openai:
    - o4-mini
    #    - gpt-4o-mini
    # - gpt-5-nano
  #deepseek:
  #        - deepseek-chat
  #huggingface:
  #       - "Qwen/Qwen2.5-7B-Instruct"
  #- NousResearch/Hermes-4-405B

#Supported model providers are: together, azure_openai, groq, ibm, azure_ai, deepseek, bedrock, google_anthropic_vertex, google_genai, o
#â”‚llama, cohere, xai, huggingface, perplexity, anthropic, openai, fireworks, mistralai, google_vertexai, bedrock_converse

# Description classes to evaluate over, should correspond to the classes
# present in the dataset/*/nl.json files.
description-classes:
    - first
    - detailed-first

# list of feedback pipelines to evaluate over.
# - [] The empty pipeline signifies no feedback, just generate the domain once.
# - "landmark generates landmark feedback messages"
# - "validate generates validation feedback messages"
# - "random-single" will randomly select a single message from the set of 
#    proceeding feedback messages
feedback-pipelines:
    - []
    - ["landmark"]
    - ["validate"]
    - ["landmark", "validate"]
    - ["landmark", "random-single"]
    - ["validate", "random-single"]
    - ["landmark", "validate", "random-single"]

# Specifies the search heuristic to use when generating plans.
# Should be a string with valid python expression that evaluates to a number
# involving G distance from the start node and H (heuristic value).
# Only used if feedback-style is set to 'heuristic-search'.
search-heuristic:
#    - "H"
    - "G + H"

# Should the model be presented with a description of predicates from the
# problem set when initially generating the domain?
give-pred-description: [true]


